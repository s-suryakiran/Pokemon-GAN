{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T03:02:23.389038Z",
     "iopub.status.busy": "2023-02-04T03:02:23.388228Z",
     "iopub.status.idle": "2023-02-04T03:02:23.394773Z",
     "shell.execute_reply": "2023-02-04T03:02:23.393695Z",
     "shell.execute_reply.started": "2023-02-04T03:02:23.389002Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T03:02:23.896699Z",
     "iopub.status.busy": "2023-02-04T03:02:23.895664Z",
     "iopub.status.idle": "2023-02-04T03:02:23.906810Z",
     "shell.execute_reply": "2023-02-04T03:02:23.905772Z",
     "shell.execute_reply.started": "2023-02-04T03:02:23.896655Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,channels_inp,input_features):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.disc=nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels_inp,input_features,kernel_size=4,stride=2,padding=1\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(input_features,input_features*2,4,2,1),\n",
    "            self._block(input_features*2,input_features*4,4,2,1),\n",
    "            self._block(input_features*4,input_features*8,4,2,1),\n",
    "            nn.Conv2d(input_features*8,1,kernel_size=4,stride=2,padding=0),\n",
    "            nn.Sigmoid()    \n",
    "        )\n",
    "    def _block(self,in_channels,out_channels,kernalsize,stride,padding):\n",
    "        return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernalsize,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=False\n",
    "        ),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(0.2)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.disc(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T03:02:24.483989Z",
     "iopub.status.busy": "2023-02-04T03:02:24.483614Z",
     "iopub.status.idle": "2023-02-04T03:02:24.493803Z",
     "shell.execute_reply": "2023-02-04T03:02:24.492792Z",
     "shell.execute_reply.started": "2023-02-04T03:02:24.483960Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,z_dim,channels_img,input_features):\n",
    "        super(Generator,self).__init__()\n",
    "        self.net = nn.Sequential(   \n",
    "            self._block(z_dim,input_features*16,4,1,0),\n",
    "            self._block(input_features*16,input_features*8,4,2,1),\n",
    "            self._block(input_features*8,input_features*4,4,2,1),\n",
    "            self._block(input_features*4,input_features*2,4,2,1),\n",
    "            nn.ConvTranspose2d(\n",
    "                input_features*2,channels_img,kernel_size=4,stride=2,padding=1\n",
    "            ),\n",
    "            nn.Tanh()\n",
    "\n",
    "        )   \n",
    "    def _block(self,in_channels,out_channels,kernalsize,stride,padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernalsize,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T03:02:24.965928Z",
     "iopub.status.busy": "2023-02-04T03:02:24.965578Z",
     "iopub.status.idle": "2023-02-04T03:02:24.972788Z",
     "shell.execute_reply": "2023-02-04T03:02:24.971605Z",
     "shell.execute_reply.started": "2023-02-04T03:02:24.965900Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m,(nn.Conv2d,nn.ConvTranspose2d,nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data,0.0,0.02)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T03:02:49.244039Z",
     "iopub.status.busy": "2023-02-04T03:02:49.243677Z",
     "iopub.status.idle": "2023-02-04T03:32:12.040001Z",
     "shell.execute_reply": "2023-02-04T03:32:12.039054Z",
     "shell.execute_reply.started": "2023-02-04T03:02:49.244008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME:  20230204-030223\n",
      "[0/500--Loss(D):0.6914--Loss(G):0.7604\n",
      "[1/500--Loss(D):0.5183--Loss(G):0.9999\n",
      "[2/500--Loss(D):0.3929--Loss(G):1.1910\n",
      "[3/500--Loss(D):0.3096--Loss(G):1.3733\n",
      "[4/500--Loss(D):0.2458--Loss(G):1.5566\n",
      "[5/500--Loss(D):0.1991--Loss(G):1.7266\n",
      "[6/500--Loss(D):0.1652--Loss(G):1.8647\n",
      "[7/500--Loss(D):0.1394--Loss(G):2.0066\n",
      "[8/500--Loss(D):0.1180--Loss(G):2.1484\n",
      "[9/500--Loss(D):0.1010--Loss(G):2.2731\n",
      "[10/500--Loss(D):0.0895--Loss(G):2.3950\n",
      "[11/500--Loss(D):0.0766--Loss(G):2.5123\n",
      "[12/500--Loss(D):0.0680--Loss(G):2.6272\n",
      "[13/500--Loss(D):0.0601--Loss(G):2.7371\n",
      "[14/500--Loss(D):0.0549--Loss(G):2.8259\n",
      "[15/500--Loss(D):0.0483--Loss(G):2.9482\n",
      "[16/500--Loss(D):0.0441--Loss(G):3.0481\n",
      "[17/500--Loss(D):0.0388--Loss(G):3.1436\n",
      "[18/500--Loss(D):0.0360--Loss(G):3.2353\n",
      "[19/500--Loss(D):0.0323--Loss(G):3.3219\n",
      "[20/500--Loss(D):0.0293--Loss(G):3.4113\n",
      "[21/500--Loss(D):0.0273--Loss(G):3.4950\n",
      "[22/500--Loss(D):0.0258--Loss(G):3.5761\n",
      "[23/500--Loss(D):0.0234--Loss(G):3.6572\n",
      "[24/500--Loss(D):0.0217--Loss(G):3.7347\n",
      "[25/500--Loss(D):0.0198--Loss(G):3.8086\n",
      "[26/500--Loss(D):0.0184--Loss(G):3.8771\n",
      "[27/500--Loss(D):0.0172--Loss(G):3.9459\n",
      "[28/500--Loss(D):0.0159--Loss(G):4.0167\n",
      "[29/500--Loss(D):0.0148--Loss(G):4.0802\n",
      "[30/500--Loss(D):0.0138--Loss(G):4.1394\n",
      "[31/500--Loss(D):0.0129--Loss(G):4.2010\n",
      "[32/500--Loss(D):0.0123--Loss(G):4.2593\n",
      "[33/500--Loss(D):0.0117--Loss(G):4.3134\n",
      "[34/500--Loss(D):0.0110--Loss(G):4.3783\n",
      "[35/500--Loss(D):0.0105--Loss(G):4.4361\n",
      "[36/500--Loss(D):0.0099--Loss(G):4.4969\n",
      "[37/500--Loss(D):0.0094--Loss(G):4.5375\n",
      "[38/500--Loss(D):0.0088--Loss(G):4.5745\n",
      "[39/500--Loss(D):0.0087--Loss(G):4.6248\n",
      "[40/500--Loss(D):0.0082--Loss(G):4.6681\n",
      "[41/500--Loss(D):0.0080--Loss(G):4.7331\n",
      "[42/500--Loss(D):0.0075--Loss(G):4.7662\n",
      "[43/500--Loss(D):0.0076--Loss(G):4.7779\n",
      "[44/500--Loss(D):0.0067--Loss(G):4.8585\n",
      "[45/500--Loss(D):0.0067--Loss(G):4.9249\n",
      "[46/500--Loss(D):0.0063--Loss(G):4.9565\n",
      "[47/500--Loss(D):0.0062--Loss(G):4.9801\n",
      "[48/500--Loss(D):0.0071--Loss(G):5.0491\n",
      "[49/500--Loss(D):0.1710--Loss(G):3.0294\n",
      "[50/500--Loss(D):0.1034--Loss(G):3.1891\n",
      "[51/500--Loss(D):0.0277--Loss(G):3.7728\n",
      "[52/500--Loss(D):0.0251--Loss(G):3.8143\n",
      "[53/500--Loss(D):0.0262--Loss(G):3.8210\n",
      "[54/500--Loss(D):2.3000--Loss(G):4.8037\n",
      "[55/500--Loss(D):0.6732--Loss(G):0.6815\n",
      "[56/500--Loss(D):0.4854--Loss(G):1.1768\n",
      "[57/500--Loss(D):0.2831--Loss(G):1.9048\n",
      "[58/500--Loss(D):0.3658--Loss(G):2.3382\n",
      "[59/500--Loss(D):1.5580--Loss(G):0.4760\n",
      "[60/500--Loss(D):0.2094--Loss(G):1.8362\n",
      "[61/500--Loss(D):0.1242--Loss(G):3.6289\n",
      "[62/500--Loss(D):0.4525--Loss(G):1.3797\n",
      "[63/500--Loss(D):0.2224--Loss(G):2.2787\n",
      "[64/500--Loss(D):0.1566--Loss(G):2.4047\n",
      "[65/500--Loss(D):0.1214--Loss(G):3.2146\n",
      "[66/500--Loss(D):0.6398--Loss(G):0.5430\n",
      "[67/500--Loss(D):0.4493--Loss(G):1.2539\n",
      "[68/500--Loss(D):0.7898--Loss(G):2.0198\n",
      "[69/500--Loss(D):0.1728--Loss(G):2.1200\n",
      "[70/500--Loss(D):1.1340--Loss(G):1.2490\n",
      "[71/500--Loss(D):0.3673--Loss(G):1.5022\n",
      "[72/500--Loss(D):0.2464--Loss(G):2.5308\n",
      "[73/500--Loss(D):0.3235--Loss(G):0.4112\n",
      "[74/500--Loss(D):0.3726--Loss(G):1.5297\n",
      "[75/500--Loss(D):0.5127--Loss(G):2.6581\n",
      "[76/500--Loss(D):0.8598--Loss(G):3.0352\n",
      "[77/500--Loss(D):0.2709--Loss(G):1.8298\n",
      "[78/500--Loss(D):0.7084--Loss(G):0.7951\n",
      "[79/500--Loss(D):0.3296--Loss(G):1.4800\n",
      "[80/500--Loss(D):0.3309--Loss(G):1.2483\n",
      "[81/500--Loss(D):0.7047--Loss(G):3.5149\n",
      "[82/500--Loss(D):0.3038--Loss(G):2.1546\n",
      "[83/500--Loss(D):0.3121--Loss(G):3.5401\n",
      "[84/500--Loss(D):0.3852--Loss(G):0.5460\n",
      "[85/500--Loss(D):0.3304--Loss(G):1.1950\n",
      "[86/500--Loss(D):0.4406--Loss(G):3.3417\n",
      "[87/500--Loss(D):0.4744--Loss(G):2.8361\n",
      "[88/500--Loss(D):0.3787--Loss(G):2.1735\n",
      "[89/500--Loss(D):0.5780--Loss(G):2.1293\n",
      "[90/500--Loss(D):0.3637--Loss(G):2.0365\n",
      "[91/500--Loss(D):0.5009--Loss(G):2.5231\n",
      "[92/500--Loss(D):0.6776--Loss(G):0.9713\n",
      "[93/500--Loss(D):0.3680--Loss(G):1.7386\n",
      "[94/500--Loss(D):1.1181--Loss(G):1.2665\n",
      "[95/500--Loss(D):0.5109--Loss(G):0.8482\n",
      "[96/500--Loss(D):0.4015--Loss(G):1.5041\n",
      "[97/500--Loss(D):0.9154--Loss(G):1.1302\n",
      "[98/500--Loss(D):0.5244--Loss(G):0.6769\n",
      "[99/500--Loss(D):0.3452--Loss(G):1.7799\n",
      "[100/500--Loss(D):0.5245--Loss(G):1.2999\n",
      "[101/500--Loss(D):0.6581--Loss(G):1.0201\n",
      "[102/500--Loss(D):0.4711--Loss(G):0.7168\n",
      "[103/500--Loss(D):0.3451--Loss(G):1.4422\n",
      "[104/500--Loss(D):0.5154--Loss(G):1.4428\n",
      "[105/500--Loss(D):0.4712--Loss(G):2.0161\n",
      "[106/500--Loss(D):0.4999--Loss(G):1.7144\n",
      "[107/500--Loss(D):0.5116--Loss(G):1.8085\n",
      "[108/500--Loss(D):0.4458--Loss(G):1.3593\n",
      "[109/500--Loss(D):0.4226--Loss(G):1.4931\n",
      "[110/500--Loss(D):0.5324--Loss(G):0.6332\n",
      "[111/500--Loss(D):0.4503--Loss(G):1.4061\n",
      "[112/500--Loss(D):0.4778--Loss(G):1.2317\n",
      "[113/500--Loss(D):0.6779--Loss(G):0.8645\n",
      "[114/500--Loss(D):0.5412--Loss(G):0.6873\n",
      "[115/500--Loss(D):0.5538--Loss(G):1.1749\n",
      "[116/500--Loss(D):0.6047--Loss(G):1.0932\n",
      "[117/500--Loss(D):0.5926--Loss(G):2.7235\n",
      "[118/500--Loss(D):0.5495--Loss(G):0.8225\n",
      "[119/500--Loss(D):0.4690--Loss(G):1.1944\n",
      "[120/500--Loss(D):0.5206--Loss(G):1.3565\n",
      "[121/500--Loss(D):0.4561--Loss(G):1.4168\n",
      "[122/500--Loss(D):0.5358--Loss(G):0.5761\n",
      "[123/500--Loss(D):0.4795--Loss(G):2.2408\n",
      "[124/500--Loss(D):0.6716--Loss(G):0.7027\n",
      "[125/500--Loss(D):0.7531--Loss(G):0.8189\n",
      "[126/500--Loss(D):0.6463--Loss(G):0.8618\n",
      "[127/500--Loss(D):0.5060--Loss(G):1.1955\n",
      "[128/500--Loss(D):0.5297--Loss(G):0.9888\n",
      "[129/500--Loss(D):0.6938--Loss(G):0.6513\n",
      "[130/500--Loss(D):0.5278--Loss(G):1.2451\n",
      "[131/500--Loss(D):0.5647--Loss(G):0.6366\n",
      "[132/500--Loss(D):0.5424--Loss(G):1.4268\n",
      "[133/500--Loss(D):0.4661--Loss(G):0.8031\n",
      "[134/500--Loss(D):0.5406--Loss(G):2.2895\n",
      "[135/500--Loss(D):0.5440--Loss(G):1.2477\n",
      "[136/500--Loss(D):0.6399--Loss(G):2.3975\n",
      "[137/500--Loss(D):0.3894--Loss(G):1.5884\n",
      "[138/500--Loss(D):0.4661--Loss(G):1.7970\n",
      "[139/500--Loss(D):0.8108--Loss(G):2.3997\n",
      "[140/500--Loss(D):0.5001--Loss(G):1.6913\n",
      "[141/500--Loss(D):0.4954--Loss(G):0.8412\n",
      "[142/500--Loss(D):0.5341--Loss(G):1.2144\n",
      "[143/500--Loss(D):0.6007--Loss(G):1.6354\n",
      "[144/500--Loss(D):0.5556--Loss(G):1.0250\n",
      "[145/500--Loss(D):0.6015--Loss(G):1.8415\n",
      "[146/500--Loss(D):0.4769--Loss(G):1.4594\n",
      "[147/500--Loss(D):0.5000--Loss(G):2.1916\n",
      "[148/500--Loss(D):0.4618--Loss(G):1.4538\n",
      "[149/500--Loss(D):0.4986--Loss(G):2.0373\n",
      "[150/500--Loss(D):0.4687--Loss(G):1.1203\n",
      "[151/500--Loss(D):0.9266--Loss(G):2.2133\n",
      "[152/500--Loss(D):0.4574--Loss(G):1.3622\n",
      "[153/500--Loss(D):0.5693--Loss(G):0.8707\n",
      "[154/500--Loss(D):0.4851--Loss(G):1.3743\n",
      "[155/500--Loss(D):0.4464--Loss(G):1.2541\n",
      "[156/500--Loss(D):0.5574--Loss(G):0.8551\n",
      "[157/500--Loss(D):0.5394--Loss(G):1.1761\n",
      "[158/500--Loss(D):0.4824--Loss(G):1.1551\n",
      "[159/500--Loss(D):0.6234--Loss(G):2.7145\n",
      "[160/500--Loss(D):0.5183--Loss(G):1.5075\n",
      "[161/500--Loss(D):0.4816--Loss(G):1.6172\n",
      "[162/500--Loss(D):0.4953--Loss(G):1.1780\n",
      "[163/500--Loss(D):0.4319--Loss(G):1.4814\n",
      "[164/500--Loss(D):0.5112--Loss(G):1.1407\n",
      "[165/500--Loss(D):0.5956--Loss(G):1.1562\n",
      "[166/500--Loss(D):0.6168--Loss(G):1.6556\n",
      "[167/500--Loss(D):0.4575--Loss(G):1.2344\n",
      "[168/500--Loss(D):0.6100--Loss(G):1.3386\n",
      "[169/500--Loss(D):0.4825--Loss(G):1.2011\n",
      "[170/500--Loss(D):0.5115--Loss(G):0.8940\n",
      "[171/500--Loss(D):0.5000--Loss(G):1.2249\n",
      "[172/500--Loss(D):0.5395--Loss(G):0.7623\n",
      "[173/500--Loss(D):0.6148--Loss(G):0.7874\n",
      "[174/500--Loss(D):0.5783--Loss(G):1.3206\n",
      "[175/500--Loss(D):0.4648--Loss(G):1.4590\n",
      "[176/500--Loss(D):0.5248--Loss(G):1.0865\n",
      "[177/500--Loss(D):0.5738--Loss(G):1.4759\n",
      "[178/500--Loss(D):0.5313--Loss(G):1.9272\n",
      "[179/500--Loss(D):0.6391--Loss(G):0.6373\n",
      "[180/500--Loss(D):0.6306--Loss(G):0.8326\n",
      "[181/500--Loss(D):0.5005--Loss(G):1.0334\n",
      "[182/500--Loss(D):0.4931--Loss(G):1.2953\n",
      "[183/500--Loss(D):0.5132--Loss(G):1.3357\n",
      "[184/500--Loss(D):0.5088--Loss(G):1.6993\n",
      "[185/500--Loss(D):0.4914--Loss(G):1.8801\n",
      "[186/500--Loss(D):0.4723--Loss(G):1.3001\n",
      "[187/500--Loss(D):0.4300--Loss(G):1.2623\n",
      "[188/500--Loss(D):0.4905--Loss(G):0.9174\n",
      "[189/500--Loss(D):0.5237--Loss(G):1.0603\n",
      "[190/500--Loss(D):0.5236--Loss(G):1.1495\n",
      "[191/500--Loss(D):0.5033--Loss(G):1.0940\n",
      "[192/500--Loss(D):0.5154--Loss(G):1.5822\n",
      "[193/500--Loss(D):0.5535--Loss(G):1.1176\n",
      "[194/500--Loss(D):0.6215--Loss(G):1.9898\n",
      "[195/500--Loss(D):0.5379--Loss(G):1.3809\n",
      "[196/500--Loss(D):0.4888--Loss(G):1.5707\n",
      "[197/500--Loss(D):0.5600--Loss(G):1.8981\n",
      "[198/500--Loss(D):0.4733--Loss(G):1.8491\n",
      "[199/500--Loss(D):0.4878--Loss(G):1.3489\n",
      "[200/500--Loss(D):0.5833--Loss(G):0.7456\n",
      "[201/500--Loss(D):0.4540--Loss(G):1.1352\n",
      "[202/500--Loss(D):0.5537--Loss(G):1.2770\n",
      "[203/500--Loss(D):0.6851--Loss(G):2.0473\n",
      "[204/500--Loss(D):0.6049--Loss(G):1.1380\n",
      "[205/500--Loss(D):0.5974--Loss(G):0.6693\n",
      "[206/500--Loss(D):0.5085--Loss(G):1.5710\n",
      "[207/500--Loss(D):0.6393--Loss(G):1.4432\n",
      "[208/500--Loss(D):0.4473--Loss(G):1.0434\n",
      "[209/500--Loss(D):0.5189--Loss(G):1.2609\n",
      "[210/500--Loss(D):0.5772--Loss(G):0.8610\n",
      "[211/500--Loss(D):0.5033--Loss(G):1.1375\n",
      "[212/500--Loss(D):0.4747--Loss(G):1.3119\n",
      "[213/500--Loss(D):0.5070--Loss(G):1.3891\n",
      "[214/500--Loss(D):0.7040--Loss(G):0.6393\n",
      "[215/500--Loss(D):0.5760--Loss(G):1.0965\n",
      "[216/500--Loss(D):0.5522--Loss(G):1.1883\n",
      "[217/500--Loss(D):0.6050--Loss(G):1.3217\n",
      "[218/500--Loss(D):0.5000--Loss(G):1.0586\n",
      "[219/500--Loss(D):0.5179--Loss(G):1.3292\n",
      "[220/500--Loss(D):0.6655--Loss(G):1.2114\n",
      "[221/500--Loss(D):0.6595--Loss(G):0.8683\n",
      "[222/500--Loss(D):0.5760--Loss(G):1.2430\n",
      "[223/500--Loss(D):0.5863--Loss(G):1.5045\n",
      "[224/500--Loss(D):0.6753--Loss(G):0.5578\n",
      "[225/500--Loss(D):0.5949--Loss(G):0.7651\n",
      "[226/500--Loss(D):0.5338--Loss(G):0.8340\n",
      "[227/500--Loss(D):0.5547--Loss(G):1.1011\n",
      "[228/500--Loss(D):0.5325--Loss(G):0.6233\n",
      "[229/500--Loss(D):0.5138--Loss(G):1.3136\n",
      "[230/500--Loss(D):0.6063--Loss(G):1.2385\n",
      "[231/500--Loss(D):0.4729--Loss(G):1.3793\n",
      "[232/500--Loss(D):0.5454--Loss(G):0.8997\n",
      "[233/500--Loss(D):0.4523--Loss(G):1.6796\n",
      "[234/500--Loss(D):0.5419--Loss(G):1.4483\n",
      "[235/500--Loss(D):0.5137--Loss(G):1.0903\n",
      "[236/500--Loss(D):0.6033--Loss(G):0.9821\n",
      "[237/500--Loss(D):0.5665--Loss(G):1.1014\n",
      "[238/500--Loss(D):0.4092--Loss(G):1.3969\n",
      "[239/500--Loss(D):0.5293--Loss(G):1.2215\n",
      "[240/500--Loss(D):0.5976--Loss(G):1.0395\n",
      "[241/500--Loss(D):0.5202--Loss(G):1.2538\n",
      "[242/500--Loss(D):0.5981--Loss(G):1.3095\n",
      "[243/500--Loss(D):0.5405--Loss(G):0.9035\n",
      "[244/500--Loss(D):0.5258--Loss(G):0.8118\n",
      "[245/500--Loss(D):0.5464--Loss(G):0.8153\n",
      "[246/500--Loss(D):0.5516--Loss(G):1.4203\n",
      "[247/500--Loss(D):0.4656--Loss(G):1.2541\n",
      "[248/500--Loss(D):0.6785--Loss(G):1.0947\n",
      "[249/500--Loss(D):0.5355--Loss(G):1.0197\n",
      "[250/500--Loss(D):0.5286--Loss(G):1.4167\n",
      "[251/500--Loss(D):0.5807--Loss(G):0.7509\n",
      "[252/500--Loss(D):0.5306--Loss(G):0.9688\n",
      "[253/500--Loss(D):0.5285--Loss(G):1.0165\n",
      "[254/500--Loss(D):0.5726--Loss(G):0.8670\n",
      "[255/500--Loss(D):0.4888--Loss(G):1.2790\n",
      "[256/500--Loss(D):0.5490--Loss(G):1.0246\n",
      "[257/500--Loss(D):0.5178--Loss(G):1.3454\n",
      "[258/500--Loss(D):0.5816--Loss(G):0.8651\n",
      "[259/500--Loss(D):0.6336--Loss(G):0.8833\n",
      "[260/500--Loss(D):0.6456--Loss(G):1.0570\n",
      "[261/500--Loss(D):0.6161--Loss(G):1.4441\n",
      "[262/500--Loss(D):0.6694--Loss(G):0.4819\n",
      "[263/500--Loss(D):0.5599--Loss(G):1.4785\n",
      "[264/500--Loss(D):0.4942--Loss(G):1.2978\n",
      "[265/500--Loss(D):0.5016--Loss(G):1.2372\n",
      "[266/500--Loss(D):0.5762--Loss(G):1.1068\n",
      "[267/500--Loss(D):0.5975--Loss(G):2.1458\n",
      "[268/500--Loss(D):0.6043--Loss(G):1.7064\n",
      "[269/500--Loss(D):0.5991--Loss(G):1.1711\n",
      "[270/500--Loss(D):0.6515--Loss(G):1.5507\n",
      "[271/500--Loss(D):0.5246--Loss(G):1.2533\n",
      "[272/500--Loss(D):0.4909--Loss(G):1.3700\n",
      "[273/500--Loss(D):0.5088--Loss(G):1.2183\n",
      "[274/500--Loss(D):0.5524--Loss(G):1.3743\n",
      "[275/500--Loss(D):0.5587--Loss(G):1.4896\n",
      "[276/500--Loss(D):0.6360--Loss(G):1.1686\n",
      "[277/500--Loss(D):0.5662--Loss(G):0.9592\n",
      "[278/500--Loss(D):0.5666--Loss(G):1.5333\n",
      "[279/500--Loss(D):0.5032--Loss(G):0.8501\n",
      "[280/500--Loss(D):0.4954--Loss(G):1.2064\n",
      "[281/500--Loss(D):0.5672--Loss(G):0.6214\n",
      "[282/500--Loss(D):0.4543--Loss(G):1.4437\n",
      "[283/500--Loss(D):0.5368--Loss(G):1.2717\n",
      "[284/500--Loss(D):0.6632--Loss(G):0.7646\n",
      "[285/500--Loss(D):0.5882--Loss(G):1.1708\n",
      "[286/500--Loss(D):0.4915--Loss(G):1.5245\n",
      "[287/500--Loss(D):0.5341--Loss(G):0.6516\n",
      "[288/500--Loss(D):0.5344--Loss(G):0.9689\n",
      "[289/500--Loss(D):0.5480--Loss(G):1.4221\n",
      "[290/500--Loss(D):0.6956--Loss(G):0.6485\n",
      "[291/500--Loss(D):0.4725--Loss(G):1.3686\n",
      "[292/500--Loss(D):0.5750--Loss(G):0.8527\n",
      "[293/500--Loss(D):0.4699--Loss(G):1.3499\n",
      "[294/500--Loss(D):0.4560--Loss(G):1.1626\n",
      "[295/500--Loss(D):0.5028--Loss(G):0.9782\n",
      "[296/500--Loss(D):0.5157--Loss(G):1.1936\n",
      "[297/500--Loss(D):0.4238--Loss(G):1.2485\n",
      "[298/500--Loss(D):0.5479--Loss(G):0.7334\n",
      "[299/500--Loss(D):0.6374--Loss(G):1.4632\n",
      "[300/500--Loss(D):0.4579--Loss(G):1.0501\n",
      "[301/500--Loss(D):0.4882--Loss(G):0.8657\n",
      "[302/500--Loss(D):0.5271--Loss(G):1.2655\n",
      "[303/500--Loss(D):0.5096--Loss(G):1.3409\n",
      "[304/500--Loss(D):0.5151--Loss(G):1.4127\n",
      "[305/500--Loss(D):0.5619--Loss(G):0.6567\n",
      "[306/500--Loss(D):0.5747--Loss(G):1.0538\n",
      "[307/500--Loss(D):0.3966--Loss(G):1.2287\n",
      "[308/500--Loss(D):0.5006--Loss(G):1.6955\n",
      "[309/500--Loss(D):0.4869--Loss(G):1.3716\n",
      "[310/500--Loss(D):0.4781--Loss(G):0.9101\n",
      "[311/500--Loss(D):0.5355--Loss(G):1.1203\n",
      "[312/500--Loss(D):0.5316--Loss(G):0.9108\n",
      "[313/500--Loss(D):0.4982--Loss(G):1.5799\n",
      "[314/500--Loss(D):0.5355--Loss(G):1.0705\n",
      "[315/500--Loss(D):0.4732--Loss(G):2.0914\n",
      "[316/500--Loss(D):0.4848--Loss(G):1.1439\n",
      "[317/500--Loss(D):0.5973--Loss(G):0.8753\n",
      "[318/500--Loss(D):0.5092--Loss(G):1.0707\n",
      "[319/500--Loss(D):0.5175--Loss(G):1.3802\n",
      "[320/500--Loss(D):0.5542--Loss(G):1.4597\n",
      "[321/500--Loss(D):0.5749--Loss(G):1.0163\n",
      "[322/500--Loss(D):0.5847--Loss(G):1.3063\n",
      "[323/500--Loss(D):0.4743--Loss(G):1.4442\n",
      "[324/500--Loss(D):0.3877--Loss(G):1.7787\n",
      "[325/500--Loss(D):0.5705--Loss(G):1.0059\n",
      "[326/500--Loss(D):0.5221--Loss(G):1.2703\n",
      "[327/500--Loss(D):0.5506--Loss(G):1.2879\n",
      "[328/500--Loss(D):0.5316--Loss(G):1.1863\n",
      "[329/500--Loss(D):0.5933--Loss(G):1.0082\n",
      "[330/500--Loss(D):0.4637--Loss(G):1.0989\n",
      "[331/500--Loss(D):0.5925--Loss(G):1.0241\n",
      "[332/500--Loss(D):0.5019--Loss(G):1.5388\n",
      "[333/500--Loss(D):0.4775--Loss(G):0.9425\n",
      "[334/500--Loss(D):0.6015--Loss(G):0.9884\n",
      "[335/500--Loss(D):0.6761--Loss(G):1.4285\n",
      "[336/500--Loss(D):0.5163--Loss(G):1.0356\n",
      "[337/500--Loss(D):0.5087--Loss(G):1.0159\n",
      "[338/500--Loss(D):0.4812--Loss(G):0.8296\n",
      "[339/500--Loss(D):0.5783--Loss(G):1.0996\n",
      "[340/500--Loss(D):0.3975--Loss(G):1.3242\n",
      "[341/500--Loss(D):0.4337--Loss(G):1.2050\n",
      "[342/500--Loss(D):0.5503--Loss(G):1.6727\n",
      "[343/500--Loss(D):0.5235--Loss(G):1.2874\n",
      "[344/500--Loss(D):0.4073--Loss(G):1.1643\n",
      "[345/500--Loss(D):0.5430--Loss(G):0.8407\n",
      "[346/500--Loss(D):0.6486--Loss(G):0.9000\n",
      "[347/500--Loss(D):0.5441--Loss(G):1.6229\n",
      "[348/500--Loss(D):0.4992--Loss(G):1.3922\n",
      "[349/500--Loss(D):0.5102--Loss(G):1.0005\n",
      "[350/500--Loss(D):0.5219--Loss(G):1.3708\n",
      "[351/500--Loss(D):0.4403--Loss(G):1.5124\n",
      "[352/500--Loss(D):0.4332--Loss(G):1.3948\n",
      "[353/500--Loss(D):0.5202--Loss(G):1.4218\n",
      "[354/500--Loss(D):0.6926--Loss(G):1.6511\n",
      "[355/500--Loss(D):0.4164--Loss(G):1.1085\n",
      "[356/500--Loss(D):0.5695--Loss(G):1.4266\n",
      "[357/500--Loss(D):0.5967--Loss(G):2.0238\n",
      "[358/500--Loss(D):0.4585--Loss(G):1.5275\n",
      "[359/500--Loss(D):0.5655--Loss(G):1.2339\n",
      "[360/500--Loss(D):0.4325--Loss(G):1.2387\n",
      "[361/500--Loss(D):0.4512--Loss(G):1.3079\n",
      "[362/500--Loss(D):0.4943--Loss(G):1.2504\n",
      "[363/500--Loss(D):0.4698--Loss(G):1.4069\n",
      "[364/500--Loss(D):0.5361--Loss(G):1.2887\n",
      "[365/500--Loss(D):0.4596--Loss(G):1.6026\n",
      "[366/500--Loss(D):0.5119--Loss(G):1.5387\n",
      "[367/500--Loss(D):0.4787--Loss(G):1.5270\n",
      "[368/500--Loss(D):0.4037--Loss(G):1.4657\n",
      "[369/500--Loss(D):0.3639--Loss(G):1.4834\n",
      "[370/500--Loss(D):0.4506--Loss(G):0.9147\n",
      "[371/500--Loss(D):0.4881--Loss(G):1.4386\n",
      "[372/500--Loss(D):0.4177--Loss(G):1.3387\n",
      "[373/500--Loss(D):0.4337--Loss(G):1.4136\n",
      "[374/500--Loss(D):0.4134--Loss(G):1.6318\n",
      "[375/500--Loss(D):0.4999--Loss(G):1.8848\n",
      "[376/500--Loss(D):0.5214--Loss(G):1.0135\n",
      "[377/500--Loss(D):0.4174--Loss(G):1.3560\n",
      "[378/500--Loss(D):0.4415--Loss(G):1.8333\n",
      "[379/500--Loss(D):0.4368--Loss(G):1.5230\n",
      "[380/500--Loss(D):0.3523--Loss(G):1.4254\n",
      "[381/500--Loss(D):0.3663--Loss(G):1.7488\n",
      "[382/500--Loss(D):0.3541--Loss(G):2.0472\n",
      "[383/500--Loss(D):0.3795--Loss(G):1.3158\n",
      "[384/500--Loss(D):0.5135--Loss(G):1.0566\n",
      "[385/500--Loss(D):0.4278--Loss(G):1.7766\n",
      "[386/500--Loss(D):0.3932--Loss(G):1.5717\n",
      "[387/500--Loss(D):0.6443--Loss(G):0.5181\n",
      "[388/500--Loss(D):0.4323--Loss(G):1.3936\n",
      "[389/500--Loss(D):0.3756--Loss(G):1.3879\n",
      "[390/500--Loss(D):0.6509--Loss(G):1.6531\n",
      "[391/500--Loss(D):0.6629--Loss(G):0.9666\n",
      "[392/500--Loss(D):0.3923--Loss(G):1.6155\n",
      "[393/500--Loss(D):0.4283--Loss(G):1.2969\n",
      "[394/500--Loss(D):0.4523--Loss(G):1.7499\n",
      "[395/500--Loss(D):0.3726--Loss(G):1.2196\n",
      "[396/500--Loss(D):0.3670--Loss(G):1.7556\n",
      "[397/500--Loss(D):0.3937--Loss(G):1.5468\n",
      "[398/500--Loss(D):0.4405--Loss(G):0.9476\n",
      "[399/500--Loss(D):0.6589--Loss(G):1.9560\n",
      "[400/500--Loss(D):0.3694--Loss(G):1.8386\n",
      "[401/500--Loss(D):0.3762--Loss(G):1.1498\n",
      "[402/500--Loss(D):0.4173--Loss(G):1.8887\n",
      "[403/500--Loss(D):0.5906--Loss(G):0.9368\n",
      "[404/500--Loss(D):0.4942--Loss(G):0.7455\n",
      "[405/500--Loss(D):0.7172--Loss(G):1.8344\n",
      "[406/500--Loss(D):0.4001--Loss(G):1.0965\n",
      "[407/500--Loss(D):0.3701--Loss(G):1.9752\n",
      "[408/500--Loss(D):0.4360--Loss(G):1.1041\n",
      "[409/500--Loss(D):0.3975--Loss(G):0.9989\n",
      "[410/500--Loss(D):0.3175--Loss(G):1.2637\n",
      "[411/500--Loss(D):0.3510--Loss(G):1.3141\n",
      "[412/500--Loss(D):0.3551--Loss(G):1.3031\n",
      "[413/500--Loss(D):0.4128--Loss(G):1.5942\n",
      "[414/500--Loss(D):0.3396--Loss(G):1.2063\n",
      "[415/500--Loss(D):0.5348--Loss(G):1.9277\n",
      "[416/500--Loss(D):0.6551--Loss(G):0.8282\n",
      "[417/500--Loss(D):0.6246--Loss(G):1.0888\n",
      "[418/500--Loss(D):0.4905--Loss(G):1.4994\n",
      "[419/500--Loss(D):0.3415--Loss(G):1.4992\n",
      "[420/500--Loss(D):0.3606--Loss(G):1.9302\n",
      "[421/500--Loss(D):0.5512--Loss(G):1.1487\n",
      "[422/500--Loss(D):0.4557--Loss(G):1.1356\n",
      "[423/500--Loss(D):0.4399--Loss(G):1.3667\n",
      "[424/500--Loss(D):0.5237--Loss(G):2.9253\n",
      "[425/500--Loss(D):0.4603--Loss(G):1.3077\n",
      "[426/500--Loss(D):0.7952--Loss(G):0.3349\n",
      "[427/500--Loss(D):0.6291--Loss(G):1.5985\n",
      "[428/500--Loss(D):0.3165--Loss(G):1.5370\n",
      "[429/500--Loss(D):0.3482--Loss(G):1.5176\n",
      "[430/500--Loss(D):0.3543--Loss(G):1.4679\n",
      "[431/500--Loss(D):0.3957--Loss(G):1.8969\n",
      "[432/500--Loss(D):0.4544--Loss(G):0.9522\n",
      "[433/500--Loss(D):0.2500--Loss(G):1.8138\n",
      "[434/500--Loss(D):0.4065--Loss(G):1.4849\n",
      "[435/500--Loss(D):0.3830--Loss(G):1.5055\n",
      "[436/500--Loss(D):0.3973--Loss(G):1.4853\n",
      "[437/500--Loss(D):0.3991--Loss(G):1.5910\n",
      "[438/500--Loss(D):0.4149--Loss(G):1.4124\n",
      "[439/500--Loss(D):0.4304--Loss(G):1.8181\n",
      "[440/500--Loss(D):0.2293--Loss(G):1.8772\n",
      "[441/500--Loss(D):0.5446--Loss(G):2.2758\n",
      "[442/500--Loss(D):0.5103--Loss(G):1.3883\n",
      "[443/500--Loss(D):0.3208--Loss(G):1.3788\n",
      "[444/500--Loss(D):0.4523--Loss(G):1.3995\n",
      "[445/500--Loss(D):0.4022--Loss(G):1.4710\n",
      "[446/500--Loss(D):0.2837--Loss(G):1.5655\n",
      "[447/500--Loss(D):0.4432--Loss(G):1.0646\n",
      "[448/500--Loss(D):0.2845--Loss(G):1.6964\n",
      "[449/500--Loss(D):0.4683--Loss(G):1.8677\n",
      "[450/500--Loss(D):0.3989--Loss(G):2.0424\n",
      "[451/500--Loss(D):0.3102--Loss(G):1.9753\n",
      "[452/500--Loss(D):0.2678--Loss(G):1.8322\n",
      "[453/500--Loss(D):0.3186--Loss(G):1.7721\n",
      "[454/500--Loss(D):0.5019--Loss(G):1.8562\n",
      "[455/500--Loss(D):0.3339--Loss(G):1.8450\n",
      "[456/500--Loss(D):0.3708--Loss(G):1.4587\n",
      "[457/500--Loss(D):0.3378--Loss(G):1.7380\n",
      "[458/500--Loss(D):0.3285--Loss(G):1.6780\n",
      "[459/500--Loss(D):0.3070--Loss(G):1.3562\n",
      "[460/500--Loss(D):0.1558--Loss(G):2.5374\n",
      "[461/500--Loss(D):0.3392--Loss(G):1.5386\n",
      "[462/500--Loss(D):0.2451--Loss(G):2.1810\n",
      "[463/500--Loss(D):0.3781--Loss(G):0.5977\n",
      "[464/500--Loss(D):0.2745--Loss(G):1.5764\n",
      "[465/500--Loss(D):0.4225--Loss(G):1.7729\n",
      "[466/500--Loss(D):0.3020--Loss(G):1.4352\n",
      "[467/500--Loss(D):0.2334--Loss(G):1.4977\n",
      "[468/500--Loss(D):0.3581--Loss(G):1.4905\n",
      "[469/500--Loss(D):0.2848--Loss(G):1.5750\n",
      "[470/500--Loss(D):0.3438--Loss(G):1.8722\n",
      "[471/500--Loss(D):0.4634--Loss(G):1.5871\n",
      "[472/500--Loss(D):0.4881--Loss(G):1.9555\n",
      "[473/500--Loss(D):0.3259--Loss(G):1.6465\n",
      "[474/500--Loss(D):0.2036--Loss(G):1.3570\n",
      "[475/500--Loss(D):0.2680--Loss(G):1.9465\n",
      "[476/500--Loss(D):0.5567--Loss(G):1.3241\n",
      "[477/500--Loss(D):0.4643--Loss(G):2.4156\n",
      "[478/500--Loss(D):0.4650--Loss(G):1.7825\n",
      "[479/500--Loss(D):0.2238--Loss(G):1.9854\n",
      "[480/500--Loss(D):0.3640--Loss(G):1.9809\n",
      "[481/500--Loss(D):0.3761--Loss(G):1.3630\n",
      "[482/500--Loss(D):0.3595--Loss(G):1.2087\n",
      "[483/500--Loss(D):0.2165--Loss(G):1.8044\n",
      "[484/500--Loss(D):0.7164--Loss(G):1.8723\n",
      "[485/500--Loss(D):0.4761--Loss(G):1.5913\n",
      "[486/500--Loss(D):0.2436--Loss(G):1.4714\n",
      "[487/500--Loss(D):0.2982--Loss(G):1.7581\n",
      "[488/500--Loss(D):0.2293--Loss(G):1.7579\n",
      "[489/500--Loss(D):0.2704--Loss(G):1.7301\n",
      "[490/500--Loss(D):0.3626--Loss(G):1.7137\n",
      "[491/500--Loss(D):0.3546--Loss(G):1.9912\n",
      "[492/500--Loss(D):0.2096--Loss(G):1.7060\n",
      "[493/500--Loss(D):0.3021--Loss(G):1.9142\n",
      "[494/500--Loss(D):0.4637--Loss(G):2.4117\n",
      "[495/500--Loss(D):0.2824--Loss(G):1.8222\n",
      "[496/500--Loss(D):0.5136--Loss(G):1.9028\n",
      "[497/500--Loss(D):0.3174--Loss(G):1.5795\n",
      "[498/500--Loss(D):0.3384--Loss(G):1.9068\n",
      "[499/500--Loss(D):0.2408--Loss(G):1.8482\n"
     ]
    }
   ],
   "source": [
    "DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr=1e-4\n",
    "z_dim=100\n",
    "img_size=64\n",
    "channels_dim=3\n",
    "batch_size=128\n",
    "num_epochs=500\n",
    "\n",
    "features_disc=64\n",
    "features_gen=64\n",
    "\n",
    "\n",
    "disc=Discriminator(channels_dim,features_disc).to(DEVICE) \n",
    "gen=Generator(z_dim,channels_dim,features_gen).to(DEVICE)\n",
    "init_weights(disc)\n",
    "init_weights(gen)\n",
    "\n",
    "fixed_noise=torch.randn(32,z_dim,1,1).to(DEVICE)\n",
    "transforms_img= transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64,64)), #transforms.Resize(IMAGE_SIZE) resizes propotionally\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5 for _ in range(channels_dim)], [0.5 for _ in range(channels_dim)])\n",
    "    ]\n",
    ")\n",
    "dataset = datasets.ImageFolder(root=\"./images\",transform=transforms_img)\n",
    "loader=DataLoader(dataset, batch_size=batch_size,shuffle=True)\n",
    "opt_disc=optim.Adam(disc.parameters(),lr=lr,betas=(0.5,0.999))    \n",
    "opt_gen=optim.Adam(gen.parameters(),lr=lr,betas=(0.5,0.999))\n",
    "critereon=nn.BCELoss()\n",
    "print(\"TIME: \",now.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "writer_fake=SummaryWriter(f\"runs/DCGAN/fake/\"+ now.strftime(\"%Y%m%d-%H%M%S\") + \"/\")\n",
    "writer_real=SummaryWriter(f\"runs/DCGAN/real/\"+ now.strftime(\"%Y%m%d-%H%M%S\") + \"/\")\n",
    "\n",
    "step=0\n",
    "gen.train()\n",
    "disc.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_index, (real,_) in enumerate(loader):\n",
    "        real=real.to(DEVICE)\n",
    "        noise=torch.randn((batch_size,z_dim,1,1)).to(DEVICE)\n",
    "\n",
    "        #Discriminator loss: max(log(D(real)) + log(1 - D(G(z))) )\n",
    "        fake_img=gen(noise)\n",
    "        \n",
    "        disc_real=disc(real).reshape(-1)\n",
    "        lossD_real=critereon(disc_real,torch.ones_like(disc_real))\n",
    "        disc_fake=disc(fake_img).reshape(-1)\n",
    "        lossD_fake=critereon(disc_fake,torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_fake+lossD_real)/2\n",
    "\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        #Train Generator\n",
    "        output=disc(fake_img).reshape(-1)\n",
    "        lossG=critereon(output,torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if(batch_index%100==0):\n",
    "            print(f'[{epoch}/{num_epochs}--Loss(D):{lossD:.4f}--Loss(G):{lossG:.4f}')\n",
    "\n",
    "        #start training\n",
    "        with torch.no_grad():\n",
    "            fake = gen(fixed_noise)\n",
    "            img_grid_fake=torchvision.utils.make_grid(fake[:32],normalize=True)\n",
    "            img_grid_real=torchvision.utils.make_grid(real[:32],normalize=True)\n",
    "\n",
    "            writer_fake.add_image(\n",
    "                \"Fake img1\",img_grid_fake,global_step=step\n",
    "            )\n",
    "            writer_real.add_image(\n",
    "                \"Real img1\",img_grid_real,global_step=step\n",
    "            )\n",
    "\n",
    "        step+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T03:32:12.042427Z",
     "iopub.status.busy": "2023-02-04T03:32:12.042063Z",
     "iopub.status.idle": "2023-02-04T03:32:12.222891Z",
     "shell.execute_reply": "2023-02-04T03:32:12.221914Z",
     "shell.execute_reply.started": "2023-02-04T03:32:12.042375Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(gen,'./models/gen.pt')\n",
    "torch.save(disc,'./models/disc.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
